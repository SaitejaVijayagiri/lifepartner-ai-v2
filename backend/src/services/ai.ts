import { OpenAI } from 'langchain/llms/openai';
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { PromptTemplate } from 'langchain/prompts';
import { StructuredOutputParser } from 'langchain/output_parsers';
import { z } from 'zod';

// Schema for parsing the user's prompt
const parser = StructuredOutputParser.fromZodSchema(
    z.object({
        values: z.array(z.string()).describe("Core values extracted from text"),
        traits: z.object({
            openness: z.number().min(0).max(1),
            conscientiousness: z.number().min(0).max(1),
            extraversion: z.number().min(0).max(1),
            agreeableness: z.number().min(0).max(1),
            neuroticism: z.number().min(0).max(1),
        }).describe("Big 5 personality traits estimated from text"),
        dealbreakers: z.array(z.string()).describe("Hard constraints or dealbreakers"),
        summary: z.string().describe("A concise summary of the ideal partner")
    })
);

export class AIService {
    // Union type for LLM or Chat Model
    private llm: any;

    constructor() {
        // PRIORITY 1: Google Gemini (Free Tier, True AI)
        if (process.env.GEMINI_API_KEY) {
            console.log("üöÄ Using Google Gemini Pro (Free Tier) for AI Services");
            this.llm = new ChatGoogleGenerativeAI({
                model: "gemini-pro",
                maxOutputTokens: 2048,
                apiKey: process.env.GEMINI_API_KEY,
                temperature: 0.7,
            });
        }
        // PRIORITY 2: OpenAI (Paid)
        else if (process.env.OPENAI_API_KEY) {
            console.log("üí∞ Using OpenAI GPT-4 for AI Services");
            this.llm = new OpenAI({
                temperature: 0.7,
                openAIApiKey: process.env.OPENAI_API_KEY
            });
        }
        // PRIORITY 3: Mock (Fallback)
        else {
            console.log("‚ö†Ô∏è No AI Keys found. Using Mock Mode.");
            this.llm = null;
        }
    }

    async parseUserPrompt(promptText: string) {
        const formatInstructions = parser.getFormatInstructions();

        const prompt = new PromptTemplate({
            template: "You are an expert matchmaker AI. Analyze the following request for a life partner:\n\n{prompt}\n\nExtract psychological traits, values, and dealbreakers.\n{format_instructions}",
            inputVariables: ["prompt"],
            partialVariables: { format_instructions: formatInstructions },
        });

        const input = await prompt.format({ prompt: promptText });

        // MOCK Fallback
        if (!this.llm || process.env.MOCK_AI === 'true') {
            const keywords = promptText.toLowerCase();
            let summary = "Seeker wants a partner.";
            if (keywords.includes('doctor')) summary = "Seeker specifically wants a Doctor/Medical professional.";

            return {
                values: ["growth", "kindness"],
                traits: { openness: 0.8, conscientiousness: 0.7, extraversion: 0.5, agreeableness: 0.9, neuroticism: 0.2 },
                dealbreakers: ["smoking"],
                summary
            };
        }

        // Call LLM
        // Note: ChatModels return BaseMessage (requires .content), LLMs return string.
        // LangChain's .call on ChatModel DOES return string if using LLMChain, but direct .call returns Message.
        // However, let's use `.invoke` or deal with the result type.
        // Easiest fix: use `invoke` which is standard in new LangChain, or handle `.call` output.
        // Legacy `OpenAI` class returns string on `.call`.
        // `ChatGoogleGenerativeAI` returns BaseMessageChunk on `.invoke`.

        // Let's create a helper to standardize.
        let responseString = "";
        try {
            if (this.llm instanceof ChatGoogleGenerativeAI) {
                const res = await this.llm.invoke(input);
                responseString = typeof res.content === 'string' ? res.content : JSON.stringify(res.content);
            } else {
                responseString = await this.llm.call(input);
            }
        } catch (e) {
            console.error("LLM Call Failed", e);
            throw e;
        }

        return await parser.parse(responseString);
    }

    // Unified Helper for Dual-Stack AI (Gemini / OpenAI)
    private async callLLM(inputProps: string | any[]): Promise<string> {
        if (!this.llm) throw new Error("AI Service not configured");

        try {
            if (this.llm instanceof ChatGoogleGenerativeAI) {
                const res = await this.llm.invoke(inputProps);
                return typeof res.content === 'string' ? res.content : JSON.stringify(res.content);
            } else {
                // OpenAI Legacy
                // OpenAI class in LangChain doesn't support array content easily in .call()
                // Use .predict() or check type.
                if (typeof inputProps !== 'string') {
                    throw new Error("OpenAI Legacy Model does not support Multimodal Input (Images).");
                }
                return await this.llm.call(inputProps);
            }
        } catch (e) {
            console.error("LLM Execution Failed", e);
            throw e;
        }
    }

    // New: Multimodal Analysis (Images)
    async analyzeImage(imageBuffer: Buffer, promptText: string) {
        if (!this.llm) return { vibe: "Mocked Visual", tags: ["Visual", "Mock"], summary: "AI not configured." };

        // Convert Buffer to Base64
        const base64Image = imageBuffer.toString('base64');

        // Construct Multimodal Message (LangChain format for Gemini)
        // For ChatGoogleGenerativeAI, we pass a HumanMessage with content array
        const { HumanMessage } = await import('langchain/schema');

        const message = new HumanMessage({
            content: [
                { type: "text", text: promptText },
                {
                    type: "image_url",
                    image_url: `data:image/jpeg;base64,${base64Image}`
                }
            ] as any // Bypass strict union check for now
        });

        try {
            const res = await this.llm.invoke([message]);
            const responseText = typeof res.content === 'string' ? res.content : JSON.stringify(res.content);

            // Reuse Vibe Parser Logic? Or custom.
            // Let's reuse the Vibe structure.
            // We need to parse strict JSON from the text.
            // Usually Gemini is good at JSON if prompted.

            // Quick Regex Manual Parse if structured parser is too rigid for raw text
            // But let's try to use the parser if we can. 
            // Actually, I can just return the raw text or try to find JSON substring.

            return responseText;
        } catch (e) {
            console.error("Image Analysis Failed", e);
            return null;
        }
    }

    // Real Compatibility Analysis
    async analyzeCompatibility(userProfile: any, matchProfile: any) {
        // ... schema ...
        const compatibilityParser = StructuredOutputParser.fromZodSchema(
            z.object({
                score: z.number().min(0).max(100).describe("Compatibility score from 0 to 100"),
                reason: z.string().describe("Brief explanation of why they match (or don't)"),
                icebreaker: z.string().describe("A fun, personalized conversation starter"),
            })
        );

        // MOCK Fallback
        if (!this.llm || process.env.MOCK_AI === 'true') {
            return {
                score: Math.floor(Math.random() * 30) + 70, // 70-100 random
                reason: "You both seem to have great energy! (Mock Analysis)",
                icebreaker: "Ask them about their favorite travel destination!"
            };
        }

        const formatInstructions = compatibilityParser.getFormatInstructions();
        const prompt = new PromptTemplate({
            template: `Analyze compatibility between two people based on their profiles.
            
            User A: {user_bio}
            User B: {match_bio}
            
            Determine a compatibility score, a reason, and a good icebreaker.
            {format_instructions}`,
            inputVariables: ["user_bio", "match_bio"],
            partialVariables: { format_instructions: formatInstructions },
        });

        const input = await prompt.format({
            user_bio: JSON.stringify(userProfile),
            match_bio: JSON.stringify(matchProfile)
        });

        try {
            const response = await this.callLLM(input);
            return await compatibilityParser.parse(response);
        } catch (e) {
            console.error("AI Analysis Failed", e);
            return {
                score: 50,
                reason: "AI matching unavailable.",
                icebreaker: "Hi!"
            };
        }
    }

    // Parse Search Query for Matching
    async parseSearchQuery(queryText: string) {
        // Schema for search filters
        const searchParser = StructuredOutputParser.fromZodSchema(
            z.object({
                profession: z.string().optional().describe("Job title or role to look for"),
                minIncome: z.number().optional().describe("Minimum annual income in LPA (Numbers only, e.g. 10)"),
                location: z.string().optional().describe("City or State preference (e.g. Hyderabad, Mumbai)"),
                minAge: z.number().optional().describe("Minimum age"),
                maxAge: z.number().optional().describe("Maximum age"),
                maritalStatus: z.string().optional().describe("Marital Status (Never Married, Divorced, Widowed)"),
                minHeightInches: z.number().optional().describe("Minimum height in inches (e.g. 5'0 = 60)"),
                maxHeightInches: z.number().optional().describe("Maximum height in inches"),
                smoking: z.enum(["Yes", "No"]).optional(),
                drinking: z.enum(["Yes", "No"]).optional(),
                diet: z.enum(["Veg", "Non-Veg", "Vegan"]).optional(),
                religion: z.string().optional().describe("Religion (Hindu, Muslim, Christian, etc.)"),
                caste: z.string().optional().describe("Specific caste or community (e.g. Brahmin, Iyer, Rajput)"),
                gothra: z.string().optional().describe("Gothra if specified"),
                education: z.string().optional().describe("Degree or College (e.g. B.Tech, IIT, MBA)"),
                familyValues: z.string().optional().describe("Family values (e.g. Traditional, Moderate, Orthodox)"),
                appearance: z.array(z.string()).describe("Physical appearance keywords (e.g. 'fair', 'tall', 'athletic')"),
                keywords: z.array(z.string()).describe("Interests/Hobbies keywords (e.g. 'hiking', 'reading', 'music')"),
                useMyLocation: z.boolean().optional().describe("True if user explicitly asks for 'near me', 'nearby', or 'local' matches")
            })
        );

        // MOCK LOGIC (Fallback / Offline Mode)
        // Improved Regex Parser to handle basic queries even without AI
        if (process.env.MOCK_AI === 'true' || !this.llm) {
            const lower = queryText.toLowerCase();
            const result: any = { keywords: [], appearance: [] };

            // 1. Profession
            if (lower.includes('software') || lower.includes('engineer') || lower.includes('developer') || lower.includes('coder')) result.profession = "Software Engineer";
            else if (lower.includes('doctor') || lower.includes('medic') || lower.includes('dr')) result.profession = "Doctor";
            else if (lower.includes('business') || lower.includes('entrepreneur')) result.profession = "Business";

            // 2. Age Range (e.g. "24-28", "25 years", "under 30")
            const ageRange = lower.match(/(\d+)\s*[-to]+\s*(\d+)/);
            if (ageRange) {
                result.minAge = parseInt(ageRange[1]);
                result.maxAge = parseInt(ageRange[2]);
            } else {
                const ageUnder = lower.match(/(?:under|below)\s*(\d+)/);
                if (ageUnder) result.maxAge = parseInt(ageUnder[1]);

                const ageOver = lower.match(/(?:over|above)\s*(\d+)/);
                if (ageOver) result.minAge = parseInt(ageOver[1]);
            }

            // 3. Simple Keywords/Interests extraction
            const commonHobbies = ['hiking', 'reading', 'travel', 'cooking', 'music', 'dance', 'movies', 'fitness', 'gym'];
            result.keywords = commonHobbies.filter(h => lower.includes(h));

            // 4. Height
            if (lower.includes('tall')) result.minHeightInches = 70;
            if (lower.includes('short')) result.maxHeightInches = 64;

            // 5. Income
            const incomeMatch = lower.match(/(\d+)\s*lpa/);
            if (incomeMatch) result.minIncome = parseInt(incomeMatch[1]);
            if (lower.includes('rich') || lower.includes('wealthy')) result.minIncome = 20;

            // 6. Location
            const cities = ['mumbai', 'delhi', 'bangalore', 'hyderabad', 'chennai', 'pune', 'usa', 'dubai', 'london'];
            const loc = cities.find(c => lower.includes(c));
            if (loc) result.location = loc.charAt(0).toUpperCase() + loc.slice(1);

            // Proximity Detection
            if (lower.includes('near me') || lower.includes('nearby') || lower.includes('close to me') || lower.includes('local') || lower.includes('my location')) {
                result.useMyLocation = true;
            }

            // 7. Diet
            if (lower.includes('vegetarian') || (lower.includes('veg') && !lower.includes('non-veg'))) result.diet = "Veg";
            else if (lower.includes('non-veg') || lower.includes('chicken') || lower.includes('meat')) result.diet = "Non-Veg";

            // 8. Marital Status
            if (lower.includes('divorced')) result.maritalStatus = "Divorced";
            if (lower.includes('widow')) result.maritalStatus = "Widowed";
            if (lower.includes('single') || lower.includes('never married')) result.maritalStatus = "Never Married";

            // 9. Explicit Height (e.g. 5'10, 5.10, 6ft)
            const heightMatch = lower.match(/(\d+)'(\d+)|(\d+)ft\s*(\d+)?|(\d+)\.(\d+)/);
            if (heightMatch) {
                // Parse 5'10 or 5.10
                const ft = parseInt(heightMatch[1] || heightMatch[3] || heightMatch[5]);
                const inches = parseInt(heightMatch[2] || heightMatch[4] || heightMatch[6] || "0");
                result.minHeightInches = (ft * 12) + inches;
            }

            // 10. Habits
            if (lower.includes('smokes') || lower.includes('smoking')) result.smoking = "Yes";
            if (lower.includes('no smoking') || lower.includes('non smoker')) result.smoking = "No";

            if (lower.includes('drink') || lower.includes('alcohol')) result.drinking = "Yes";
            if (lower.includes('teetotaller') || lower.includes('no drink')) result.drinking = "No";

            return result;
        }

        // Real AI Logic
        const formatInstructions = searchParser.getFormatInstructions();
        const prompt = new PromptTemplate({
            template: `You are an expert Indian Matchmaker AI. 
Convert the user's natural language search request into structured search filters.

Rules:
1. **Job Titles**: Map informal terms to standard professions (e.g. "Coder" -> "Software Engineer", "Doc" -> "Doctor").
2. **Height**: "Tall" = minHeightInches 70 (5'10"). "Short" = maxHeightInches 64 (5'4").
3. **Income**: "High earning", "Rich", "Well settled" = minIncome 20 (LPA).
4. **Location**: Extract City or State clearly.
5. **Religions**: Normalize to "Hindu", "Muslim", "Christian", "Sikh", "Jain".
6. **Age**: Extract minAge and maxAge (e.g. "25-30", "under 30", "above 25").

Request: "{query}"

{format_instructions}`,
            inputVariables: ["query"],
            partialVariables: { format_instructions: formatInstructions },
        });

        const input = await prompt.format({ query: queryText });
        const response = await this.callLLM(input);
        return await searchParser.parse(response);
    }

    // ... embedding logic remains same ...

    // ... embedding logic remains same ...
    async generateEmbedding(text: string): Promise<number[]> {
        return Array(1536).fill(0).map(() => Math.random());
    }

    // Voice Safety: Local Transcription (Free)
    async transcribeAudio(filePath: string): Promise<string> {
        try {
            console.log(`üéôÔ∏è Transcribing: ${filePath}`);

            // Dynamic import to handle ESM/CommonJS compat
            const { pipeline } = await import('@xenova/transformers');

            const transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny');
            const output = await transcriber(filePath);
            const transcript = (output as any).text || "";

            console.log(`üìù Transcript: "${transcript}"`);
            return transcript;

        } catch (e) {
            console.error("Transcription Error", e);
            return "";
        }
    }

    // Phase 2: Analyze Personality Vibe from Transcript
    async analyzePersonalityFromText(text: string) {
        if (!text || text.length < 5) {
            return { vibe: "Mysterious", tags: ["Quiet"], confidence: 0.5, summary: "Not enough audio to analyze." };
        }

        const vibeParser = StructuredOutputParser.fromZodSchema(
            z.object({
                vibe: z.string().describe("Two word vibe, e.g. 'Calm Intellectual' or 'Bubbly Extrovert'"),
                tags: z.array(z.string()).describe("3-5 personality keywords"),
                confidence: z.number().describe("0-1 confidence score"),
                summary: z.string().describe("1 sentence psychological summary")
            })
        );

        // MOCK Fallback (Only if NO Gemini AND NO OpenAI)
        if (!this.llm || process.env.MOCK_AI === 'true') {
            return {
                vibe: "Mocked Intelligent",
                tags: ["Smart", "Articulate", "Mock"],
                confidence: 0.85,
                summary: "This is a mock analysis of the transcript."
            };
        }

        const formatInstructions = vibeParser.getFormatInstructions();
        const prompt = new PromptTemplate({
            template: "Analyze the psychology of this person based on their voice transcript:\n\n\"{text}\"\n\nDetermine their 'Vibe', traits, and write a summary.\n{format_instructions}",
            inputVariables: ["text"],
            partialVariables: { format_instructions: formatInstructions },
        });

        const input = await prompt.format({ text });
        try {
            const response = await this.callLLM(input);
            return await vibeParser.parse(response);
        } catch (e) {
            console.error("AI Personality Analysis Failed", e);
            return { vibe: "Unknown", tags: ["Unknown"], confidence: 0, summary: "Analysis failed." };
        }
    }

    // Phase 2: Multiplayer Game (Relationship Scenarios)
    async generateRelationshipScenario(profileA: any, profileB: any) {
        if (!this.llm) {
            return {
                title: "The Lost Wallet",
                description: "You find a wallet with $500 but no ID. Person A wants to keep it, Person B wants to donate it.",
                options: ["Keep it", "Donate it", "Police"]
            };
        }

        const prompt = `
            Generate a relationship conflict scenario for two people to solve together.
            
            Person A: ${JSON.stringify(profileA)}
            Person B: ${JSON.stringify(profileB)}
            
            Create a "What would you do?" situation that tests their values (Spending vs Saving, Family vs Career, etc.).
            Tailor it to their specific traits (e.g. if one is adventurous, maybe a travel mishap).
            
            Return JSON:
            {
                "title": "Short Title",
                "description": "The scenario description (2-3 sentences)",
                "options": ["Option A", "Option B", "Option C"]
            }
        `;

        try {
            const res = await this.callLLM(prompt);
            // Quick clean in case of markdown
            const clean = res.replace(/```json/g, '').replace(/```/g, '').trim();
            return JSON.parse(clean);
        } catch (e) {
            console.error("Scenario Gen Failed", e);
            return {
                title: "Dinner Plans",
                description: "You can't decide where to eat.",
                options: ["Pizza", "Sushi", "Cook at home"]
            };
        }
    }

    async evaluateCooperation(chatTranscript: string[]) {
        if (!this.llm) return { score: 85, feedback: "Good effort (Mock)." };

        const prompt = `
            Analyze this chat transcript between a couple solving a problem.
            Evaluate their communication style, empathy, and compromise.
            
            Transcript:
            ${chatTranscript.join('\n')}
            
            Return JSON:
            {
                "score": 85, (0-100)
                "feedback": "One sentence feedback on their dynamic."
            }
        `;

        try {
            const res = await this.callLLM(prompt);
            const clean = res.replace(/```json/g, '').replace(/```/g, '').trim();
            return JSON.parse(clean);
        } catch (e) {
            return { score: 50, feedback: "Analysis failed." };
        }
    }

    // Phase 3: Deep Reporting
    async generateDeepAnalysis(profileA: any, profileB: any) {
        if (!this.llm) {
            return `
# Executive Summary
Matching Score: 85%
These two individuals show strong potential for a long-term partnership based on shared values.

# Emotional Compatibility
User A's conscientiousness complements User B's openness, suggesting a balance of order and adventure.

# Shared Values
Both prioritize Family and Growth.

# Conflict Zones
Potential friction around "Friday Nights" (Introvert vs Extrovert).

# Growth Advice
Focus on scheduled quality time.
            `;
        }

        const prompt = `
            You are a senior relationship psychologist. Write a detailed compatibility report for:
            
            Partner A: ${JSON.stringify(profileA)}
            Partner B: ${JSON.stringify(profileB)}
            
            Structure the report in Markdown format with these exact headers:
            # Executive Summary
            # Emotional Compatibility
            # Shared Values
            # Conflict Zones
            # Growth Advice
            
            Tone: Professional, Insightful, Encouraging but Realistic.
            Length: 400-600 words.
        `;

        try {
            const res = await this.callLLM(prompt);
            return res;
        } catch (e) {
            console.error("Deep Analysis Failed", e);
            return "Analysis Unavailable.";
        }
    }
}
